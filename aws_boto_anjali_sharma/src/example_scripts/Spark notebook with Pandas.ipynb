{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "8cb16da0",
			"metadata": {},
			"source": [
				"### This notebook shows examples of using Glue on Spark for users familiar with pandas\n",
				"To follow this example notebook, execute the cells in order.\n",
				"The keyboard shortcut to execute the current cell and jump to the following is: Shift+Enter.\n",
				"\n",
				"To delete cells no longer needed (including this one), you can use the context menu or use the Escape key (to exit any cell you might be in) and then press the d key twice. You can select multiple cells using Shift + Up/Down, to delete many quickly.\n",
				"\n",
				"This example assumes the configured role has permission to read/write on the default catalog database and the s3 glue temporary folder, otherwise update the code or the permissions accordingly."
			]
		},
		{
			"cell_type": "markdown",
			"id": "dc7cacd1",
			"metadata": {},
			"source": [
				"####  Running the following cell will set up and start your interactive session."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a3fea417",
			"metadata": {},
			"outputs": [],
			"source": [
				"%idle_timeout 120\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n",
				"\n",
				"import boto3\n",
				"import sys\n",
				"from awsglue.dynamicframe import DynamicFrame\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from pyspark.sql.functions import *\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "7a4db5b0",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Optimize the data movement from pandas to Spark DataFrame and back\n",
				"spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
				"\n",
				"# You can define a distributed Spark DataFrame, to read the data in a distributed way and be able to process large data\n",
				"# Here it takes a bit of time because we ask it to infer schema, in practice could just let it set everything as string\n",
				"# and handle the schema manually\n",
				"sdf = spark.read.csv(\"s3://awsglue-datasets/examples/medicare/Medicare_Hospital_Provider.csv\", \n",
				"                     header=True, inferSchema=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "7f867e92",
			"metadata": {},
			"outputs": [],
			"source": [
				"# The schema inference considered the dollar amounts as string due to the $ symbol\n",
				"# Also in the csv there are some header with extra spaces, we'll deal with that later\n",
				"sdf.printSchema()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "473bf9ac",
			"metadata": {},
			"outputs": [],
			"source": [
				"# The last 3 columns are dollar amounts, let's parse them into Decimal numbers for calculations\n",
				"last_3cols = sdf.columns[-3:]\n",
				"# These transformations are just defined here, until we extract the data Spark won't do the work (lazy execution)\n",
				"for col_name in last_3cols:\n",
				"    # Note: normally for monies it's better to use decimal but pandas doesn't support it\n",
				"    sdf = sdf.withColumn(col_name, regexp_replace(sdf[col_name], '\\$', '').cast('double'))\n",
				"\n",
				"# The zip code is not really a number\n",
				"sdf = sdf.withColumn('Provider Zip Code', sdf['Provider Zip Code'].cast('string'))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "68372c4f",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Check the parsing is working fine\n",
				"sdf.show(n=10)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "150b2e81",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Let's say you are only interested in California\n",
				"sdf_ca = sdf.filter('`Provider State` == \"CA\"')\n",
				"\n",
				"# Now that we have narrowed down the data, it's small enough that we can convert into a native pandas DataFrame\n",
				"# Unlike sdf which reads data distributed and when needed, this pdf uses the driver memory to store the data\n",
				"# so is faster for smaller data as long as it fits\n",
				"pdf_ca = sdf_ca.toPandas()\n",
				"\n",
				"# The column names in the csv have extra spaces, in pandas we can trim that easily\n",
				"pdf_ca.columns = [c.strip() for c in pdf_ca.columns]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "4d69ff44",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Check the pandas schema\n",
				"pdf_ca.dtypes"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "42882f97",
			"metadata": {
				"scrolled": false
			},
			"outputs": [],
			"source": [
				"# Explore the statistics of the numeric columns\n",
				"import pandas as pd\n",
				"import numpy as np\n",
				"pd.set_option('display.max_columns', 5)\n",
				"pd.set_option('max_colwidth', 30)\n",
				"pdf_ca.describe()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d0310bfc",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Plot a histogram on the notebook directly from pandas\n",
				"import matplotlib.pyplot as plt\n",
				"plt.figure()\n",
				"plt.title(\"Histogram of average Medicare payments\")\n",
				"plt.xlabel(\"Average payment in dollars\")\n",
				"histogram = pdf_ca['Average Medicare Payments'].plot.hist(bins=12, alpha=0.5)\n",
				"%matplot plt"
			]
		},
		{
			"cell_type": "markdown",
			"id": "fcb425ef",
			"metadata": {},
			"source": [
				"If you have more experience with the pandas APIs would rather use that instead of Spark DataFrame  \n",
				"But notice that while we use native pandas, only the driver was doing work and the rest of the cluster is not used   \n",
				"That's why we set the minimum size: *%number_of_workers 2*  \n",
				"\n",
				"On Glue 4.0, you can get both distributed computed and the pandas syntax by using the \"pandas on Spark\" API, it's not yet 100% compatible but should work for most cases"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a74e182e",
			"metadata": {},
			"outputs": [],
			"source": [
				"# The following cells will only work on Glue 4.0\n",
				"# psdf is a pandas on Spark DataFrame, uses the pandas API but the data and processing is distributed\n",
				"# this means it has higher latency but also can scale beyond a single node to handler larger data\n",
				"psdf = sdf.pandas_api()\n",
				"psdf.columns = [c.strip() for c in psdf.columns]\n",
				"# Statistics on the full dataset\n",
				"psdf.describe()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "fc32263a",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Operate the data in a distributed way but using pandas syntax\n",
				"relevant_psdf = psdf[(psdf['Total Discharges'] > 100) & (psdf['Average Medicare Payments'] > 10000)]\n",
				"relevant_psdf.groupby([\"Provider State\", \"Provider Id\"])[\"Average Medicare Payments\"].max()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "78801bf9",
			"metadata": {},
			"source": [
				"Note that in the previous cell the output is not equivalent than the same on native pandas, which displays the results grouped by the first column.   \n",
				"Also running a sort before the groupby wouldn't work the same way, in this case the data is distributed so it's unsorted again when doing the groupby. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a264ce60",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Convert back to Spark DataFrame if you want leverage the data saving features (for instance creating a catalog table)\n",
				"# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html\n",
				"relevant_sdf = relevant_psdf.to_spark()\n",
				"relevant_sdf.show()\n",
				"\n",
				"# Or go a step further and convert to DynamicFrame to use its sinks and features\n",
				"# https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame-writer.html\n",
				"relevant_dynf = DynamicFrame.fromDF(relevant_sdf, glueContext, \"\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}