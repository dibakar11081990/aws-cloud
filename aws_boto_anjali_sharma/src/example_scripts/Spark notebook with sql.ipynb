{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "c887bb54",
			"metadata": {},
			"source": [
				"### This notebook shows examples of using SQL to explore and handle data\n",
				"To follow this example notebook, execute the cells in order.  \n",
				"The keyboard shortcut to execute the current cell and jump to the following is: Shift+Enter.\n",
				"\n",
				"To delete cells no longer needed (including this one), you can use the context menu or use the Escape key (to exit any cell you might be in) and then press the d key twice. You can select multiple cells using Shift + Up/Down, to delete many quickly.  \n",
				"\n",
				"This example assumes the configured role has permission to read/write on the default catalog database and the s3 glue temporary folder, otherwise update the code or the permissions accordingly."
			]
		},
		{
			"cell_type": "markdown",
			"id": "72061504",
			"metadata": {},
			"source": [
				"####  Running the following cell will set up and start your interactive session."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "c0bc868c",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%idle_timeout 120\n",
				"%glue_version 3.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 3\n",
				"\n",
				"import boto3\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from pyspark.sql.functions import *\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "41aa72ee",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%%sql\n",
				"-- This is a SQL cell running against the account Glue catalog in the same region\n",
				"-- One query per cell\n",
				"show databases"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "977ec91c",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%%sql\n",
				"-- You can use ANSI SQL syntax to explore the catalog and run queries that print the results\n",
				"-- You can even run DDL to make changes, such as ALTER TABLE\n",
				"-- If your database or table has special characters, you can escape the name with a backtick `\n",
				"-- for instance: SELECT * FROM `mydb`.`mytable` LIMIT 10\n",
				"show tables in default"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "9a78e418",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# This is a Pyspark cell (which is the default)\n",
				"# In the previous cell, tables with more than 20 characters in the name would be truncated\n",
				"# or if you have more than 20 tables, some are not displayed\n",
				"# Here we do the same but using directly the API to have more control:\n",
				"spark.sql(\"show tables in default\").show(n=30, truncate=False)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "38bd4ad1",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# If you data doesn't have a table in the catalog, you can use a temporary view to use SQL\n",
				"# Here we read all the CSV files under the indicated s3 path\n",
				"medicareDF = spark.read.csv(\"s3://awsglue-datasets/examples/medicare/\", header=True)\n",
				"# If there data has a reasonable size (like in this case), we can cache in memory/disk (depending on cluster size) \n",
				"# so after the first query, the following no longer have to go to read and parse from s3\n",
				"medicareDF.cache()\n",
				"# Instead of using the DataFrame API, you can register it as a view for SQL usage like this:\n",
				"medicareDF.registerTempTable(\"medicare\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f6c168f1",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Explore the data, since it has many long columns, change the display to be vertical for easier read\n",
				"spark.sql(\"SELECT * from medicare\").show(n=10, truncate=False, vertical=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "0aec2be6",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# You can also register as a view the result of another query, to avoid repetition\n",
				"spark.sql(\"SELECT * FROM medicare WHERE `Provider State` = 'NY'\").registerTempTable(\"ny_medicare\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d232a5c0",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# You can retrieve the results of the query into a Python variable if it's small enough\n",
				"# For example, retrieve the Diagnostric Related Group with the highest average payments in New York\n",
				"diagnostic_group = spark.sql(\"\"\"\n",
				"SELECT DISTINCT FIRST_VALUE(`DRG Definition`)  OVER (ORDER BY `Average Medicare Payments` DESC) AS drg\n",
				"FROM ny_medicare\n",
				"\"\"\").collect()[0]['drg']\n",
				"print(diagnostic_group)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "6c75a5a8",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# By default this example will save using the glue temporary bucket, you can replace it for another of your choice\n",
				"output_bucket = f\"aws-glue-temporary-{boto3.client('sts').get_caller_identity()['Account']}-{boto3.session.Session().region_name}\"\n",
				"\n",
				"provider_summary_df = spark.sql(\"\"\"\n",
				"SELECT `Provider Id`, `Provider Name`, sum(` Total Discharges `) as `Total Discharges` \n",
				"FROM medicare GROUP BY `Provider Id`, `Provider Name`\n",
				"\"\"\")\n",
				"# Save as a single CSV file with headers under the indicated s3 path\n",
				"# Be careful, mode overwrite will wipe the ouput directory before writing\n",
				"# The parenthesis are an alternative to \\ to break a Python command into multiple lines\n",
				"(provider_summary_df.coalesce(1).write\n",
				"    .mode(\"overwrite\")\n",
				"    .csv(f\"s3://{output_bucket}/example/medicare_by_provider_summary/\", header=True)\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "dcb264e5",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Or save it as a parquet table on the catalog, we also rename the columns for the table\n",
				"(provider_summary_df\n",
				"    .withColumnRenamed(\"Provider Id\", \"provider_id\")\n",
				"    .withColumnRenamed(\"Provider Name\", \"provider_name\")\n",
				"    .withColumnRenamed(\"Total Discharges\", \"total_discharges\")\n",
				"    .write\n",
				"    .mode(\"overwrite\")\n",
				"    .option(\"path\", f\"s3://{output_bucket}/example/medicare_by_provider_summary_table/\")\n",
				"    .format(\"parquet\")\n",
				"    .saveAsTable(\"default.example_medicare_by_provider_summary\")\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d416e49c",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%%sql\n",
				"-- Check the new catalog table\n",
				"DESCRIBE TABLE default.example_medicare_by_provider_summary"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "34a8d286",
			"metadata": {
				"scrolled": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Free the cache when no longer needed \n",
				"medicareDF.unpersist()\n",
				"\n",
				"# Remove the table (but not the s3 files)\n",
				"spark.sql(\"DROP TABLE default.example_medicare_by_provider_summary\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "aff80288",
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
